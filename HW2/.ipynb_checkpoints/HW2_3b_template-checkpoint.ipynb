{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import load_cifar_template as lct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_1,training_labels_1 = pickle.load(open('preprocess_batch_1.p','rb'))\n",
    "training_features_2,training_labels_2 = pickle.load(open('preprocess_batch_2.p','rb'))\n",
    "training_features_3,training_labels_3 = pickle.load(open('preprocess_batch_3.p','rb'))\n",
    "training_features_4,training_labels_4 = pickle.load(open('preprocess_batch_4.p','rb'))\n",
    "training_features_5,training_labels_5 = pickle.load(open('preprocess_batch_5.p','rb'))\n",
    "\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p','rb'))\n",
    "test_features, test_labels = pickle.load(open('preprocess_testing.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.vstack([training_features_1, training_features_2, training_features_3, \n",
    "                           training_features_4, training_features_5])\n",
    "train_labels = np.vstack([training_labels_1, training_labels_2, training_labels_3, \n",
    "                         training_labels_4, training_labels_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = 0.001\n",
    "#number of epochs\n",
    "epochs=10\n",
    "#number of batch_size\n",
    "batch_size=64\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "he_init=tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"X\")\n",
    "Y = tf.placeholder(tf.int64, [None, num_classes], name=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LeNet-5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(X):\n",
    "    # Here we defind the CNN architecture (LeNet-5)\n",
    "\n",
    "    # Reshape input to 4-D vector\n",
    "    input_layer = tf.reshape(X, [-1, 32, 32, 3]) # -1 adds minibatch support.\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Has a default stride of 1\n",
    "    # Output: 28 * 28 * 6\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=6, # Number of filters.\n",
    "      kernel_size=5, # Size of each filter is 5x5.\n",
    "      padding=\"valid\", # No padding is applied to the input.\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # Sampling half the output of previous layer\n",
    "    # Output: 14 * 14 * 6\n",
    "    pool1 = tf.layers.average_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Output: 10 * 10 * 16\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=16, # Number of filters\n",
    "      kernel_size=5, # Size of each filter is 5x5\n",
    "      padding=\"valid\", # No padding\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Output: 5 * 5 * 16\n",
    "    pool2 = tf.layers.average_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Reshaping output into a single dimention array for input to fully connected layer\n",
    "    #pool2_flat = tf.reshape(pool2, [-1, 5 * 5 * 16])\n",
    "    pool2_flat = tf.layers.flatten(pool2)\n",
    "\n",
    "    # Fully connected layer #1: Has 120 neurons\n",
    "    dense1 = tf.layers.dense(inputs=pool2_flat, units=120, activation=tf.nn.tanh,kernel_initializer=he_init)\n",
    "\n",
    "    # Fully connected layer #2: Has 84 neurons\n",
    "    dense2 = tf.layers.dense(inputs=dense1, units=84, activation=tf.nn.tanh,kernel_initializer=he_init)\n",
    "\n",
    "    # Output layer, 10 neurons for each digit\n",
    "    logits = tf.layers.dense(inputs=dense2, units=10, kernel_initializer=he_init)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and Optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-5b7354bb7b27>:15: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Jimin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-5b7354bb7b27>:20: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.average_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-5b7354bb7b27>:37: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-5b7354bb7b27>:40: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "logits = CNN(X)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "# Convert our labels into one-hot-vectors\n",
    "#labels = tf.one_hot(indices=tf.cast(Y, tf.int32), depth=10)\n",
    "\n",
    "# Compute the cross-entropy loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                 labels=Y))\n",
    "\n",
    "# Use adam optimizer to reduce cost\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "# For testing and prediction\n",
    "predictions = tf.argmax(softmax, axis=1)\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# Initialize all the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training, validation and testing</h1>\n",
    "<h2>Train your model only 10 epochs.</h2>\n",
    "<h2>1.Print out validation accuracy after each training epoch</h2>\n",
    "<h2>2.Print out training time for each training epoch</h2>\n",
    "<h2>3.Print out testing accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 16.806282 s\n",
      "Epoch 1: Cost: 1.667781287973577\n",
      "Validation accuracy: 0.47440001368522644\n",
      "Took 16.837348 s\n",
      "Epoch 2: Cost: 1.4240838365459996\n",
      "Validation accuracy: 0.4851999878883362\n",
      "Took 16.923164 s\n",
      "Epoch 3: Cost: 1.3416528298773542\n",
      "Validation accuracy: 0.527400016784668\n",
      "Took 16.849493 s\n",
      "Epoch 4: Cost: 1.2674140223217294\n",
      "Validation accuracy: 0.5371999740600586\n",
      "Took 16.893342 s\n",
      "Epoch 5: Cost: 1.2127844030037538\n",
      "Validation accuracy: 0.5613999962806702\n",
      "Took 16.936068 s\n",
      "Epoch 6: Cost: 1.1634912009096954\n",
      "Validation accuracy: 0.5842000246047974\n",
      "Took 16.954651 s\n",
      "Epoch 7: Cost: 1.1180413997816774\n",
      "Validation accuracy: 0.5834000110626221\n",
      "Took 16.942995 s\n",
      "Epoch 8: Cost: 1.079606527885931\n",
      "Validation accuracy: 0.5830000042915344\n",
      "Took 16.910387 s\n",
      "Epoch 9: Cost: 1.0453544561751191\n",
      "Validation accuracy: 0.5982000231742859\n",
      "Took 16.889115 s\n",
      "Epoch 10: Cost: 1.0147906497798183\n",
      "Validation accuracy: 0.6092000007629395\n",
      "Testing accuracy: 0.6133000254631042\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    " \n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        num_samples = train_features.shape[0]\n",
    "        num_batches = (num_samples // batch_size) + 1\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        # Shuffle training data each epoch\n",
    "        shuffle_index = np.random.permutation(num_samples)\n",
    "        train_features_shuffled = train_features[shuffle_index]\n",
    "        train_labels_shuffled = train_labels[shuffle_index]\n",
    "        \n",
    "        i = 0\n",
    "        while i < num_samples:\n",
    "            batch_x = train_features_shuffled[i:i+batch_size,:]\n",
    "            batch_y = train_labels_shuffled[i:i+batch_size]\n",
    "\n",
    "            i += batch_size\n",
    "\n",
    "            # Train on batch and get back cost\n",
    "            _, c = sess.run([train_op, cost], feed_dict={X:batch_x, Y:batch_y})\n",
    "            epoch_cost += (c/num_batches)\n",
    "            \n",
    "            #print(epoch_cost)\n",
    "            \n",
    "        end = time.time()\n",
    "        print(\"Took %f s\" % ((end - start)))\n",
    "\n",
    "        # Get accuracy for validation\n",
    "        valid_accuracy = accuracy.eval(\n",
    "            feed_dict={X:valid_features, Y:valid_labels})\n",
    "\n",
    "        print (\"Epoch {}: Cost: {}\".format(epoch+1, epoch_cost))\n",
    "        print(\"Validation accuracy: {}\".format(valid_accuracy))\n",
    "\n",
    "    test_accuracy = accuracy.eval(feed_dict={X:test_features, Y:test_labels})\n",
    "    \n",
    "    print(\"Testing accuracy: {}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
