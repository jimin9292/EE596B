{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load training, validation, testing set from your preprocessed files</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = pickle.load(open('cat_dog_train.p','rb'))\n",
    "\n",
    "x_valid, y_valid = pickle.load(open('cat_dog_valid.p','rb'))\n",
    "x_test = pickle.load(open('cat_dog_test.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define hyperparameter</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "image_width = 227\n",
    "image_height = 227\n",
    "image_depth = 3\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Placeholder</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>AlexNet</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlexNet(X):\n",
    "\n",
    "    # Reshape input to 4-D vector\n",
    "    input_layer = tf.reshape(X, [-1, 227, 227, 3]) # -1 adds minibatch support.\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Has a default stride of 1\n",
    "    # Output: 28 * 28 * 6\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=96, # Number of filters.\n",
    "      kernel_size=11, \n",
    "      strides=(4,4),\n",
    "      padding=\"valid\", # No padding is applied to the input.\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer=he_init)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # Sampling half the output of previous layer\n",
    "    # Output: 14 * 14 * 6\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[3, 3], strides=2)\n",
    "    pool1 = tf.nn.local_response_normalization(pool1)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Output: 10 * 10 * 16\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=256, # Number of filters\n",
    "      kernel_size=5, # Size of each filter is 5x5\n",
    "      padding=\"SAME\", # No padding\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer=he_init)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Output: 5 * 5 * 16\n",
    "    pool2 = tf.layers.average_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    pool2 = tf.nn.local_response_normalization(pool2)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=256, # Number of filters\n",
    "      kernel_size=5, # Size of each filter is 5x5\n",
    "      padding=\"SAME\", # No padding\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer=he_init)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Output: 5 * 5 * 16\n",
    "    pool2 = tf.layers.average_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    pool2 = tf.nn.local_response_normalization(pool2)\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=256, # Number of filters\n",
    "      kernel_size=5, # Size of each filter is 5x5\n",
    "      padding=\"SAME\", # No padding\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer=he_init)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Output: 5 * 5 * 16\n",
    "    pool2 = tf.layers.average_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    pool2 = tf.nn.local_response_normalization(pool2)\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=256, # Number of filters\n",
    "      kernel_size=5, # Size of each filter is 5x5\n",
    "      padding=\"SAME\", # No padding\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer=he_init)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Output: 5 * 5 * 16\n",
    "    pool2 = tf.layers.average_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    pool2 = tf.nn.local_response_normalization(pool2)\n",
    "\n",
    "    # Reshaping output into a single dimention array for input to fully connected layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 5 * 5 * 16])\n",
    "\n",
    "    # Fully connected layer #1: Has 120 neurons\n",
    "    dense1 = tf.layers.dense(inputs=pool2_flat, units=120, activation=tf.nn.relu,kernel_initializer=he_init)\n",
    "\n",
    "    # Fully connected layer #2: Has 84 neurons\n",
    "    dense2 = tf.layers.dense(inputs=dense1, units=84, activation=tf.nn.relu,kernel_initializer=he_init)\n",
    "\n",
    "    # Output layer, 10 neurons for each digit\n",
    "    logits = tf.layers.dense(inputs=dense2, units=10)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost and Optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = AlexNet(X)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "# Convert our labels into one-hot-vectors\n",
    "#labels = tf.one_hot(indices=tf.cast(Y, tf.int32), depth=10)\n",
    "\n",
    "# Compute the cross-entropy loss\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                 labels=Y))\n",
    "\n",
    "# Use adam optimizer to reduce cost\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "# For testing and prediction\n",
    "predictions = tf.argmax(softmax, axis=1)\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# Initialize all the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training and validation</h1>\n",
    "<h2>Train your model only 10 epochs</h2>\n",
    "<p style=\"font-size:20px\">1. Print out training accuracy and validation accuracy each training epoch</p>\n",
    "<p style=\"font-size:20px\">2. Print out training time each training epoch</p>\n",
    "<p style=\"font-size:20px\">3. Your goal is to reach 85% validation accuracy in 10 training epochs. If you reach that, you can perform testing, print out your test accuracy. Plot out the ten images with title that contains the probability of the labeled class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    " \n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        num_samples = train_features.shape[0]\n",
    "        num_batches = (num_samples // batch_size) + 1\n",
    "        epoch_cost = 0.\n",
    "        i = 0\n",
    "        while i < num_samples:\n",
    "            batch_x = train_features[i:i+batch_size,:]\n",
    "            batch_y = train_labels[i:i+batch_size]\n",
    "\n",
    "            i += batch_size\n",
    "\n",
    "            # Train on batch and get back cost\n",
    "            _, c = sess.run([train_op, cost], feed_dict={X:batch_x, Y:batch_y})\n",
    "            epoch_cost += (c/num_batches)\n",
    "\n",
    "        # Get accuracy for validation\n",
    "        valid_accuracy = accuracy.eval(\n",
    "            feed_dict={X:valid_features, Y:valid_labels})\n",
    "\n",
    "        print (\"Epoch {}: Cost: {}\".format(epoch+1, epoch_cost))\n",
    "        print(\"Validation accuracy: {}\".format(valid_accuracy))\n",
    "\n",
    "    test_accuracy = accuracy.eval(feed_dict={X:test_features, Y:test_labels})\n",
    "    \n",
    "    print(\"Testing accuracy: {}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
