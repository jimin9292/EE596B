{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract MNIST data</h2>\n",
    "<p style=\"font-size:20px\">You can change the option of one_hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = mnist.train.num_examples #55,000\n",
    "num_validation = mnist.validation.num_examples #5000\n",
    "num_test = mnist.test.num_examples #10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning steps\n",
    "num_steps = 1500\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 1500\n",
    "n_hidden_2 = 750\n",
    "n_hidden_3 = 375\n",
    "num_input = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define placeholder and Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jimin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1]),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]),name='W2'),\n",
    "    'W3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3]),name='W3'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_3, num_classes]),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'b3': tf.Variable(tf.zeros(shape=[n_hidden_3]),name='b3'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define neural network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.nn.relu(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "    layer_2_out = tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2'])\n",
    "    layer_3_out = tf.add(tf.matmul(layer_2_out,weights['W3']),biases['b3'])\n",
    "    out = tf.add(tf.matmul(layer_3_out,weights['Wout']),biases['bout'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define cost function and accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='loss')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Execute training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, Accuracy= 0.344\n",
      "step 500, Accuracy= 0.969\n",
      "step 1000, Accuracy= 0.977\n",
      "Training finished!\n",
      "Testing Accuracy: 0.9607\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        #fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        #run optimization\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "        if i % 500 ==0:\n",
    "            acc = sess.run(accuracy,feed_dict={X:batch_x, Y:batch_y})\n",
    "            print(\"step \"+str(i)+\", Accuracy= {:.3f}\".format(acc))\n",
    "    \n",
    "    print(\"Training finished!\")\n",
    "    \n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Your results</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parameters vs Performance table</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='hw1_P3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary of the findings</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The results above show that when varying each parameter independently while keeping others default values, switching activation function from None to Relu has the most noticeable improvement on performance. However, all the other paramters seem to show similar performance as we vary their values. Increasing number of neurons does seem to improve the performance little bit but not much, possibly due to overfitting issue. Adding additional layer also didn't improve much, but this seems to be due to not having any activation function for all three layers. Increasing number of epochs or batch sizes also had minimal effect on performance. This could be mostly due to the relatively simple nature (i.e. not very diverse) of dataset and increasing batch size may improve the training speed but doesn't really affect the performance. </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The best performance seems to be attained by changing multiple parameters. In my example, I constructed the three layers network with sizes 1500, 750 and 375, and used ReLu activation function for the first layer. The total steps were 1500 with batch size 128, or equivalently about 4 epochs. Additionally, I used AdamOptimizer instead of Gradient Descent. This way, I was able consistently get >96% performance on test data </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
